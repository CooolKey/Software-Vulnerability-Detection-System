import os
import re
import csv
import tensorflow as tf
import numpy as np
from tensorflow.contrib import learn
from data import load_data_and_filenames, data_iter


def Predict(data_dir, checkpoint_dir, document_length_limit, batch_size, is_line_as_word):
    # Restore vocab
    vocab_path = os.path.join(checkpoint_dir, "..", "vocab")
    vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)

    # Load data
    x_raw, filenames = load_data_and_filenames(data_dir, document_length_limit, is_line_as_word)
    x_test = np.array(list(vocab_processor.transform(x_raw)))

    # Evaluation
    checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)
    graph = tf.Graph()
    with graph.as_default():
        sess = tf.Session()
        with sess.as_default():
            saver = tf.train.import_meta_graph("{}.meta".format(checkpoint_file))
            saver.restore(sess, checkpoint_file)

            input_x = graph.get_operation_by_name("x_input").outputs[0]
            dropout_keep_prob = graph.get_operation_by_name("dropout_keep_prob").outputs[0]
            mode = graph.get_operation_by_name("mode").outputs[0]

            fc = graph.get_operation_by_name("fc/fc").outputs[0]
            predictions = graph.get_operation_by_name("predictions").outputs[0]

            batches = data_iter(list(x_test), batch_size, 1, shuffle=False)

            all_predictions = []
            all_fc_scores = None

            for x_test_batch in batches:
                batch_fc_score, batch_predictions = sess.run([fc, predictions], {input_x: x_test_batch,
                                                             dropout_keep_prob: 1.0,
                                                             mode: False})
                all_predictions = np.concatenate([all_predictions, batch_predictions])
                if all_fc_scores is None:
                    all_fc_scores = batch_fc_score
                else:
                    all_fc_scores = np.concatenate([all_fc_scores, batch_fc_score])

    return all_predictions


def get_document_length_limit(dir_name):
    p = re.compile('sen_len=(\d+)')
    m = p.search(dir_name)
    if m:
        return int(m.group(1))
    else:
        return None


def predict(data_dir):
    # checkpoint_dir = "system/source/runs/checkpoints"
    checkpoint_dir = "source/runs/checkpoints"
    batch_size = 64
    is_line_as_word = True
    document_length_limit = 1000

    return Predict(data_dir, checkpoint_dir, document_length_limit, batch_size, is_line_as_word)


if __name__ == "__main__":
    # data_dir = "system/source/unknow"
    data_dir = "source/unknow"
    predict(data_dir)

